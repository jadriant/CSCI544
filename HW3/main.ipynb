{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jadriantan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jadriantan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Might need this \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/jadriantan/miniconda3/lib/python3.11/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jadriantan/miniconda3/lib/python3.11/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jadriantan/miniconda3/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install bs4 # in case you don't have it installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation (from HW1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/sk_p32bx74j8lpnjdx_bmjz80000gn/T/ipykernel_43668/2324558469.py:1: DtypeWarning: Columns (1,7,17,18,19,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data.csv\", sep=',', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\", sep=',', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body star_rating\n",
      "0                                     Great product.           5\n",
      "1  What's to say about this commodity item except...           5\n",
      "2    Haven't used yet, but I am sure I will like it.           5\n",
      "3  Although this was labeled as &#34;new&#34; the...           1\n",
      "4                    Gorgeous colors and easy to use           4\n"
     ]
    }
   ],
   "source": [
    "# Extract Reviews and Ratings fields\n",
    "df = df.loc[:, ['review_body', 'star_rating']]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review_body star_rating  label  \\\n",
      "98510   These do not work for refill ink it states one...           1      1   \n",
      "233824  Once the ink cartriges were installed, things ...           3      1   \n",
      "877033  Nothing special to say here - they are postcar...           3      1   \n",
      "929541               It is a machine for counting bills.?           1      1   \n",
      "201881  I guess you get what you pay for.  The magenta...           2      1   \n",
      "456205               Not the best quality. Pale in color.           3      1   \n",
      "645041               They do what they're supposed to do.           3      1   \n",
      "862255               Very light weight, barely hold paper           1      1   \n",
      "633612                          Only for 5 sheets or less           1      1   \n",
      "736944  Not happy with these. I am not able to print w...           1      1   \n",
      "\n",
      "                                                   review  \n",
      "98510   These do not work for refill ink it states one...  \n",
      "233824  Once the ink cartriges were installed, things ...  \n",
      "877033  Nothing special to say here - they are postcar...  \n",
      "929541               It is a machine for counting bills.?  \n",
      "201881  I guess you get what you pay for.  The magenta...  \n",
      "456205               Not the best quality. Pale in color.  \n",
      "645041               They do what they're supposed to do.  \n",
      "862255               Very light weight, barely hold paper  \n",
      "633612                          Only for 5 sheets or less  \n",
      "736944  Not happy with these. I am not able to print w...  \n"
     ]
    }
   ],
   "source": [
    "# Converting into binary classification problem\n",
    "df['label'] = df['star_rating'].apply(lambda x: 1 if x in [1,2,3] else 2)\n",
    "df['review'] = df['review_body']\n",
    "\n",
    "# Selecting 50,000 random reviews from each rating class\n",
    "# Randomizing to avoid biases \n",
    "df_class_1 = df[df['label'] == 1].sample(n=50000, random_state=55)\n",
    "df_class_2 = df[df['label'] == 2].sample(n=50000, random_state=55)\n",
    "\n",
    "# Creating a new df concatenating both classes \n",
    "balanced_df = pd.concat([df_class_1, df_class_2])\n",
    "\n",
    "print(balanced_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (from HW1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length before data cleaning:189.4582, Average length after data cleaning:179.7509\n"
     ]
    }
   ],
   "source": [
    "average_len_before = balanced_df['review'].str.len().mean()\n",
    "\n",
    "# 1)converting all reviews into lowercase. \n",
    "# Ensures consistency: \"Hello\" and \"hello\" are now the same. \n",
    "balanced_df['review'] = balanced_df['review'].str.lower()\n",
    "\n",
    "# 2)removing the HTML and URLs from the reviews\n",
    "# HTML/URLs don't provide valuable information for sentiment analysis, so we remove them. \n",
    "balanced_df['review'] = balanced_df['review'].str.replace(r'<.*?>', '', regex=True)\n",
    "balanced_df['review'] = balanced_df['review'].str.replace(r'http\\S+', '', regex=True)\n",
    "\n",
    "# 5)performing contractions on the reviews\n",
    "    # Need to process this before before removing non-alphanum chars and extra spaces \n",
    "# This task provides uniformity and simplifies tokenization\n",
    "def contractions_helper(ss):\n",
    "\n",
    "    # To avoid attributionError\n",
    "    if type(ss) != str: \n",
    "        return\n",
    "    contractions_dict = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"coudn't\": \"could not\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\"\n",
    "    }\n",
    "    # loop through the string and replace all contractions\n",
    "    for cont, exp in contractions_dict.items():\n",
    "            if cont in ss:\n",
    "                 ss = ss.replace(cont, exp)\n",
    "    return ss\n",
    "\n",
    "balanced_df['review'] = balanced_df['review'].apply(contractions_helper)\n",
    "\n",
    "# 3)removing non-alphabetical characters\n",
    "# Removing non-alphanum characters since they could be noise in sentiment analysis\n",
    "balanced_df['review'] = balanced_df['review'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "\n",
    "# 4)removing extra spaces\n",
    "balanced_df['review'] = balanced_df['review'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "\n",
    "average_len_after = balanced_df['review'].str.len().mean()\n",
    "\n",
    "# average length decreased after cleaning due to the removal of unwanted characters, spaces, and expansion of contractions\n",
    "print(f'Average length before data cleaning:{average_len_before:.4f}, Average length after data cleaning:{average_len_after:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out rows where 'review' is not a string\n",
    "balanced_df = balanced_df[balanced_df['review'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Word2Vec model trained with amazon reviews\n",
    "sentences = balanced_df['review'].str.split().tolist() # get review sentences\n",
    "# train word2vec model\n",
    "my_model = Word2Vec(sentences, vector_size=300, window=13, min_count=9, workers=4)\n",
    "my_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "# Pre-trained word2vec model\n",
    "wv_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Embedding: part (a) and (b) combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Word2Vec Model: Similarity for words 'excellent' and 'outstanding': 0.55674857\n",
      "My Model: Similarity between 'excellent' and 'outstanding': 0.6556493\n",
      "Example 2\n",
      "Word2Vec Model: King - Man + Woman =  queen\n",
      "My Model: King - Man + Woman =  nurse\n",
      "Example 3 ('delicious' and 'tasty' not in my model)\n",
      "Word2Vec Model: Similarity for words 'delicious' and 'tasty': 0.873039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Example 1\")\n",
    "# Pretrained: check for semantic similarities\n",
    "print(\"Word2Vec Model: Similarity for words 'excellent' and 'outstanding':\", wv_model.similarity('excellent', 'outstanding'))\n",
    "\n",
    "# My model: check semantic similarities\n",
    "if 'excellent' in my_model.wv and 'outstanding' in my_model.wv:\n",
    "    print(\"My Model: Similarity between 'excellent' and 'outstanding':\", my_model.wv.similarity('excellent', 'outstanding'))\n",
    "\n",
    "print(\"Example 2\")\n",
    "# Pretrained: check for analogy: King - Man + Woman\n",
    "result = wv_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(\"Word2Vec Model: King - Man + Woman = \", result[0][0])\n",
    "\n",
    "# My model: check analogy: King - Man + Woman\n",
    "if all(word in my_model.wv for word in ['woman', 'king', 'man']):\n",
    "    result = my_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "    print(\"My Model: King - Man + Woman = \", result[0][0])\n",
    "\n",
    "print(\"Example 3 ('delicious' and 'tasty' not in my model)\")\n",
    "# Pretrained model:\n",
    "print(\"Word2Vec Model: Similarity for words 'delicious' and 'tasty':\", wv_model.similarity('delicious', 'tasty'))\n",
    "\n",
    "# My model:\n",
    "if 'delicious' in my_model.wv.key_to_index and 'tasty' in my_model.wv.key_to_index:\n",
    "    print(\"My Model: Similarity between 'delicious' and 'tasty':\", my_model.wv.similarity('delicious', 'tasty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Models: Perceptron and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# TFIDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TFIDF Features\n",
    "tfidf_vector = TfidfVectorizer(max_features=5000)\n",
    "tfidf_features = tfidf_vector.fit_transform(balanced_df['review'])\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
    "    tfidf_features, \n",
    "    balanced_df['label'],\n",
    "    test_size= 0.2,\n",
    "    random_state=55\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average Word2Vec for each review\n",
    "def average_word2vec(review, model, dimension):\n",
    "    avg_w2v = np.zeros((dimension,))\n",
    "    num_words = 0\n",
    "    for word in review:\n",
    "        if word in model:  \n",
    "            avg_w2v += model[word]\n",
    "            num_words += 1\n",
    "    if num_words > 0:\n",
    "        avg_w2v /= num_words\n",
    "    return avg_w2v\n",
    "\n",
    "# Splitting reviews into sentences for Word2Vec\n",
    "sentences = balanced_df['review'].str.split().tolist()\n",
    "\n",
    "# Convert reviews into feature vectors using average Word2Vec helper function\n",
    "word2vec_features = np.array([average_word2vec(review, wv_model, 300) for review in sentences])\n",
    "\n",
    "# Splitting into train and test sets\n",
    "x_train_word2vec, x_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(\n",
    "    word2vec_features, \n",
    "    balanced_df['label'],\n",
    "    test_size= 0.2,\n",
    "    random_state=55\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron Model w/ TFIDF features and word2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Perceptron Model~ Precision:0.7969, Recall:0.8345, F1-Score:0.8153\n",
      "Word2Vec Perceptron Model~ Precision:0.8374, Recall:0.7370, F1-Score:0.7840\n"
     ]
    }
   ],
   "source": [
    "# Perceptron on tfidf features\n",
    "tfidf_perceptron = Perceptron(max_iter=1000, random_state=55)\n",
    "\n",
    "# Fit training set into Perceptron Model \n",
    "tfidf_perceptron.fit(x_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Make predictions w/ testing set\n",
    "tfidf_prediction = tfidf_perceptron.predict(x_test_tfidf)\n",
    "\n",
    "# Report Precision, Recall, and f1-score\n",
    "tfidf_test_precision = precision_score(y_test_tfidf, tfidf_prediction, average='binary')\n",
    "tfidf_test_recall = recall_score(y_test_tfidf, tfidf_prediction, average='binary')\n",
    "tfidf_test_f1 = f1_score(y_test_tfidf, tfidf_prediction, average='binary')\n",
    "\n",
    "# Print\n",
    "print(f\"TF-IDF Perceptron Model~ Precision:{tfidf_test_precision:.4f}, Recall:{tfidf_test_recall:.4f}, F1-Score:{tfidf_test_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Perceptron on word2vec features\n",
    "word2vec_perceptron = Perceptron(max_iter=1000, random_state=55)\n",
    "\n",
    "# Fit training set into Perceptron Model\n",
    "word2vec_perceptron.fit(x_train_word2vec, y_train_word2vec)\n",
    "\n",
    "# Make predictions w/ testing set\n",
    "word2vec_prediction = word2vec_perceptron.predict(x_test_word2vec)\n",
    "\n",
    "# Report Precision, Recall, and f1-score\n",
    "word2vec_test_precision = precision_score(y_test_word2vec, word2vec_prediction, average='binary')\n",
    "word2vec_test_recall = recall_score(y_test_word2vec, word2vec_prediction, average='binary')\n",
    "word2vec_test_f1 = f1_score(y_test_word2vec, word2vec_prediction, average='binary')\n",
    "\n",
    "# Print\n",
    "print(f\"Word2Vec Perceptron Model~ Precision:{word2vec_test_precision:.4f}, Recall:{word2vec_test_recall:.4f}, F1-Score:{word2vec_test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Model w/ TFIDF features and word2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF SVM Model~ Precision:0.8554 Recall:0.8767 F1-Score:0.8659\n",
      "Word2Vec SVM Model~ Precision:0.7926 Recall:0.8762 F1-Score:0.8323\n"
     ]
    }
   ],
   "source": [
    "# SVM on tfidf features\n",
    "tfidf_svm = LinearSVC(dual=False, max_iter=1000, random_state=55)\n",
    "\n",
    "# Fit training set into SVM classifier \n",
    "tfidf_svm.fit(x_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Predict on testing set\n",
    "tfidf_svm_prediction = tfidf_svm.predict(x_test_tfidf)\n",
    "\n",
    "# Report Precision, Recall, and f1-score\n",
    "tfidf_svm_precision = precision_score(y_test_tfidf, tfidf_svm_prediction, average='binary')\n",
    "tfidf_svm_recall = recall_score(y_test_tfidf, tfidf_svm_prediction, average='binary')\n",
    "tfidf_svm_f1 = f1_score(y_test_tfidf, tfidf_svm_prediction, average='binary')\n",
    "\n",
    "# Print\n",
    "print(f\"TF-IDF SVM Model~ Precision:{tfidf_svm_precision:.4f} Recall:{tfidf_svm_recall:.4f} F1-Score:{tfidf_svm_f1:.4f}\")\n",
    "\n",
    "# SVM on word2vec features\n",
    "word2vec_svm = LinearSVC(dual=False, max_iter=1000, random_state=55)\n",
    "\n",
    "# Fit training set into SVM classifier\n",
    "word2vec_svm.fit(x_train_word2vec, y_train_word2vec)\n",
    "\n",
    "# Predict on testing set\n",
    "word2vec_svm_prediction = word2vec_svm.predict(x_test_word2vec)\n",
    "\n",
    "# Report Precision, Recall, and f1-score\n",
    "word2vec_svm_precision = precision_score(y_test_word2vec, word2vec_svm_prediction, average='binary')\n",
    "word2vec_svm_recall = recall_score(y_test_word2vec, word2vec_svm_prediction, average='binary')\n",
    "word2vec_svm_f1 = f1_score(y_test_word2vec, word2vec_svm_prediction, average='binary')\n",
    "\n",
    "# Print\n",
    "print(f\"Word2Vec SVM Model~ Precision:{word2vec_svm_precision:.4f} Recall:{word2vec_svm_recall:.4f} F1-Score:{word2vec_svm_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My explanation:\n",
    "- Deep learning models perform better with embeddings like Word2Vec than with sparse representations like TF-IDF\n",
    "- Linear models like Perceptron and SVM might sometimes favor the discriminative power of TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_train_tensor: tensor([1, 2])\n",
      "Unique values in y_test_tensor: tensor([1, 2])\n",
      "Shape of X_train_tensor: torch.Size([79988, 300])\n",
      "Shape of X_test_tensor: torch.Size([19998, 300])\n",
      "Updated unique values in y_train_tensor: tensor([0, 1])\n",
      "Updated unique values in y_test_tensor: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.FloatTensor(x_train_word2vec)\n",
    "y_train_tensor = torch.LongTensor(y_train_word2vec.values) \n",
    "x_test_tensor = torch.FloatTensor(x_test_word2vec)\n",
    "y_test_tensor = torch.LongTensor(y_test_word2vec.values)\n",
    "\n",
    "# Check before moving forward\n",
    "print(\"Unique values in y_train_tensor:\", torch.unique(y_train_tensor))\n",
    "print(\"Unique values in y_test_tensor:\", torch.unique(y_test_tensor))\n",
    "\n",
    "print(\"Shape of X_train_tensor:\", x_train_tensor.shape)\n",
    "# print(\"Sample values from X_train_tensor:\", X_train_tensor[:5])\n",
    "print(\"Shape of X_test_tensor:\", x_test_tensor.shape)\n",
    "# print(\"Sample values from X_test_tensor:\", X_test_tensor[:5])\n",
    "\n",
    "# Re-map the values from [1, 2] to [0, 1] for proper training\n",
    "y_train_tensor -= 1\n",
    "y_test_tensor -= 1\n",
    "\n",
    "print(\"Updated unique values in y_train_tensor:\", torch.unique(y_train_tensor))\n",
    "print(\"Updated unique values in y_test_tensor:\", torch.unique(y_test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3715682029724121\n",
      "Epoch 2/10, Loss: 0.32229456305503845\n",
      "Epoch 3/10, Loss: 0.7002264857292175\n",
      "Epoch 4/10, Loss: 0.38312360644340515\n",
      "Epoch 5/10, Loss: 0.36629727482795715\n",
      "Epoch 6/10, Loss: 0.35043641924858093\n",
      "Epoch 7/10, Loss: 0.27408459782600403\n",
      "Epoch 8/10, Loss: 0.24510405957698822\n",
      "Epoch 9/10, Loss: 0.38804447650909424\n",
      "Epoch 10/10, Loss: 0.23674869537353516\n",
      "Test Accuracy: 84.89%\n"
     ]
    }
   ],
   "source": [
    "# Create train loader with batch size 64\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1_size, hidden2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2_size, output_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Hyperparameters: two hidden layers, each with 50 and 5 nodes, respectively\n",
    "input_size = 300  \n",
    "hidden1_size = 50  \n",
    "hidden2_size = 5   \n",
    "output_size = 2    \n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = MLP(input_size, hidden1_size, hidden2_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for _, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(x_test_tensor)\n",
    "    _, predicted = test_outputs.max(1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_word2vec)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the first 10 Word2Vec vectors for each review as the input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_first_10_word2vec(review, model, dimension):\n",
    "    feature_vec = []\n",
    "    for i in range(10):\n",
    "        if i < len(review) and review[i] in model:\n",
    "            feature_vec.extend(model[review[i]])\n",
    "        else:\n",
    "            feature_vec.extend(np.zeros((dimension,)))\n",
    "    return feature_vec\n",
    "\n",
    "sentences = balanced_df['review'].str.split().tolist()\n",
    "\n",
    "# Convert reviews into feature vectors using concatenated Word2Vec\n",
    "word2vec_10words_features = np.array([concat_first_10_word2vec(review, wv_model, 300) for review in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets using word2vec_10words_features\n",
    "x_train_10words, x_test_10words, y_train_10words, y_test_10words = train_test_split(\n",
    "    word2vec_10words_features, \n",
    "    balanced_df['label'],\n",
    "    test_size= 0.2,\n",
    "    random_state=55\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.FloatTensor(x_train_10words)\n",
    "y_train_tensor = torch.LongTensor(y_train_10words.values) \n",
    "x_test_tensor = torch.FloatTensor(x_test_10words)\n",
    "y_test_tensor = torch.LongTensor(y_test_10words.values)\n",
    "\n",
    "# re-map the values from [1, 2] to [0, 1] for proper training\n",
    "y_train_tensor -= 1\n",
    "y_test_tensor -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4302016794681549\n",
      "Epoch 2/10, Loss: 0.3950132727622986\n",
      "Epoch 3/10, Loss: 0.3557981252670288\n",
      "Epoch 4/10, Loss: 0.44132062792778015\n",
      "Epoch 5/10, Loss: 0.2974357604980469\n",
      "Epoch 6/10, Loss: 0.32928821444511414\n",
      "Epoch 7/10, Loss: 0.16482013463974\n",
      "Epoch 8/10, Loss: 0.2622988820075989\n",
      "Epoch 9/10, Loss: 0.14231359958648682\n",
      "Epoch 10/10, Loss: 0.17618674039840698\n",
      "Test Accuracy: 79.17%\n"
     ]
    }
   ],
   "source": [
    "# Create train loader with batch size 64\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Hyperparameters: two hidden layers, each with 50 and 5 nodes, respectively\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1_size, hidden2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2_size, output_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Hyperparameters: two hidden layers, each with 50 and 5 nodes, respectively\n",
    "input_size = 3000  \n",
    "hidden1_size = 50  \n",
    "hidden2_size = 5   \n",
    "output_size = 2    \n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = MLP(input_size, hidden1_size, hidden2_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for _, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(x_test_tensor)\n",
    "    _, predicted = test_outputs.max(1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_10words)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My explanation:\n",
    "- The test accuracy drops after concatenating the first 10 words of each review.\n",
    "- This is because when you concatenate the first 10 Word2Vec vectors, you might be losing some information that is crucial for the classification task\n",
    "- Word2Vec vectors capture semantic meaning. By taking only the first 10 words, the model might be missing out on important context that comes from the rest of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: using x_train_word2vec, x_test_word2vec, y_train_word2vec, y_test_word2vec\n",
    "\n",
    "# Maintain the sequence of vectors since RNN processes inputs one step at a time and maintains a hidden state across those steps\n",
    "def get_word2vec_sequence(review, model, dimension):\n",
    "    w2v_sequence = []\n",
    "    for word in review:\n",
    "        if word in model:\n",
    "            w2v_sequence.append(model[word])\n",
    "        else:\n",
    "            w2v_sequence.append(np.zeros((dimension,)))  # Using a zero vector for unknown words\n",
    "    return w2v_sequence\n",
    "\n",
    "# Convert reviews into sequences of Word2Vec vectors\n",
    "sentences = balanced_df['review'].str.split().tolist()\n",
    "word2vec_sequences = [get_word2vec_sequence(review, wv_model, 300) for review in sentences]\n",
    "\n",
    "# To feed your data into our RNN, limit the maximum review length to 10 \n",
    "# by truncating longer reviews and padding shorter reviews with a null value (0)\n",
    "def pad_or_truncate_sequence(sequence, max_length):\n",
    "    if len(sequence) > max_length:\n",
    "        sequence = sequence[:max_length]\n",
    "    else:\n",
    "        while len(sequence) < max_length:\n",
    "            sequence.append(np.zeros((300,)))  # Padding with zero vectors\n",
    "    return sequence\n",
    "\n",
    "padded_word2vec_sequences = np.array([pad_or_truncate_sequence(seq, 10) for seq in word2vec_sequences])\n",
    "\n",
    "# Split into train and test sets using padded_word2vec_sequences\n",
    "x_train_word2vec, x_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(\n",
    "    padded_word2vec_sequences, \n",
    "    balanced_df['label'],\n",
    "    test_size= 0.2,\n",
    "    random_state=55\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79988, 10, 300) (19998, 10, 300)\n"
     ]
    }
   ],
   "source": [
    "# check before moving forward \n",
    "print(x_train_word2vec.shape, x_test_word2vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: one hidden layer with 10\n",
    "input_size = 300  \n",
    "hidden_size = 10\n",
    "output_size = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)  # Initialize hidden state\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Only take the last time step's output for classification\n",
    "        return out\n",
    "  \n",
    "model = SimpleRNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "x_train_rnn_tensor = torch.FloatTensor(x_train_word2vec)\n",
    "x_test_rnn_tensor = torch.FloatTensor(x_test_word2vec)\n",
    "y_train_rnn_tensor = torch.LongTensor(y_train_word2vec.values)\n",
    "y_test_rnn_tensor = torch.LongTensor(y_test_word2vec.values)\n",
    "\n",
    "# print(\"Unique values in y_train_tensor:\", torch.unique(y_train_rnn_tensor))\n",
    "# print(\"Unique values in y_test_tensor:\", torch.unique(y_test_rnn_tensor))\n",
    "\n",
    "# re-map the values from [1, 2] to [0, 1] for proper training\n",
    "y_train_rnn_tensor -= 1\n",
    "y_test_rnn_tensor -= 1\n",
    "\n",
    "# print(\"Updated unique values in y_train_tensor:\", torch.unique(y_train_rnn_tensor))\n",
    "# print(\"Updated unique values in y_test_tensor:\", torch.unique(y_test_rnn_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.505071222782135\n",
      "Epoch 2/10, Loss: 0.4083254635334015\n",
      "Epoch 3/10, Loss: 0.5449137091636658\n",
      "Epoch 4/10, Loss: 0.38038599491119385\n",
      "Epoch 5/10, Loss: 0.4555507004261017\n",
      "Epoch 6/10, Loss: 0.3713168799877167\n",
      "Epoch 7/10, Loss: 0.43283843994140625\n",
      "Epoch 8/10, Loss: 0.4200085401535034\n",
      "Epoch 9/10, Loss: 0.5686778426170349\n",
      "Epoch 10/10, Loss: 0.4306006133556366\n",
      "Test Accuracy: 78.41%\n"
     ]
    }
   ],
   "source": [
    "# Create train loader with batch size 64\n",
    "train_data_rnn = TensorDataset(x_train_rnn_tensor, y_train_rnn_tensor)\n",
    "train_loader_rnn = DataLoader(train_data_rnn, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for _, (data, target) in enumerate(train_loader_rnn):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(x_test_rnn_tensor)\n",
    "    _, predicted = test_outputs.max(1)\n",
    "    accuracy = (predicted == y_test_rnn_tensor).sum().item() / len(y_test_word2vec)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.28653085231781006\n",
      "Epoch 2/10, Loss: 0.33367300033569336\n",
      "Epoch 3/10, Loss: 0.2835990786552429\n",
      "Epoch 4/10, Loss: 0.4465479254722595\n",
      "Epoch 5/10, Loss: 0.19093047082424164\n",
      "Epoch 6/10, Loss: 0.2864411175251007\n",
      "Epoch 7/10, Loss: 0.28328919410705566\n",
      "Epoch 8/10, Loss: 0.37917467951774597\n",
      "Epoch 9/10, Loss: 0.2826038599014282\n",
      "Epoch 10/10, Loss: 0.3535066246986389\n",
      "Test Accuracy: 83.47%\n"
     ]
    }
   ],
   "source": [
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)  \n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # NOTE: Only taking the last time step's output for classification\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleGRU(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  \n",
    "\n",
    "# Training\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for _, (data, target) in enumerate(train_loader_rnn):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(x_test_rnn_tensor)\n",
    "    _, predicted = test_outputs.max(1)\n",
    "    accuracy = (predicted == y_test_rnn_tensor).sum().item() / len(y_test_word2vec)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3438650965690613\n",
      "Epoch 2/10, Loss: 0.29859182238578796\n",
      "Epoch 3/10, Loss: 0.23388096690177917\n",
      "Epoch 4/10, Loss: 0.46848025918006897\n",
      "Epoch 5/10, Loss: 0.29186028242111206\n",
      "Epoch 6/10, Loss: 0.30844613909721375\n",
      "Epoch 7/10, Loss: 0.30707797408103943\n",
      "Epoch 8/10, Loss: 0.2040465623140335\n",
      "Epoch 9/10, Loss: 0.33895012736320496\n",
      "Epoch 10/10, Loss: 0.3702055513858795\n",
      "Test Accuracy: 83.80%\n"
     ]
    }
   ],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        \n",
    "        # Using LSTM instead of GRU\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize both hidden state and cell state for LSTM\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Only take the last time step's output for classification\n",
    "        return out\n",
    "\n",
    "# Model, Criterion, Optimizer Initialization\n",
    "model = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for _, (data, target) in enumerate(train_loader_rnn):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(x_test_rnn_tensor)\n",
    "    _, predicted = test_outputs.max(1)\n",
    "    accuracy = (predicted == y_test_rnn_tensor).sum().item() / len(y_test_word2vec)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My explanation: \n",
    "- Both GRU and LSTM have significantly higher accuracies compared to the Simple RNN. \n",
    "- This underscores the effectiveness of gating mechanisms in capturing long-term dependencies and mitigating issues like vanishing and exploding gradients that simple RNNs suffer from."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
