{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jadriantan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jadriantan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Might need this \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/jadriantan/miniconda3/lib/python3.11/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jadriantan/miniconda3/lib/python3.11/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jadriantan/miniconda3/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install bs4 # in case you don't have it installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/sk_p32bx74j8lpnjdx_bmjz80000gn/T/ipykernel_28575/2324558469.py:1: DtypeWarning: Columns (1,7,17,18,19,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data.csv\", sep=',', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\", sep=',', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body star_rating\n",
      "0                                     Great product.           5\n",
      "1  What's to say about this commodity item except...           5\n",
      "2    Haven't used yet, but I am sure I will like it.           5\n",
      "3  Although this was labeled as &#34;new&#34; the...           1\n",
      "4                    Gorgeous colors and easy to use           4\n"
     ]
    }
   ],
   "source": [
    "# Extract Reviews and Ratings fields\n",
    "df = df.loc[:, ['review_body', 'star_rating']]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review_body star_rating  label  \\\n",
      "98510   These do not work for refill ink it states one...           1      1   \n",
      "233824  Once the ink cartriges were installed, things ...           3      1   \n",
      "877033  Nothing special to say here - they are postcar...           3      1   \n",
      "929541               It is a machine for counting bills.?           1      1   \n",
      "201881  I guess you get what you pay for.  The magenta...           2      1   \n",
      "456205               Not the best quality. Pale in color.           3      1   \n",
      "645041               They do what they're supposed to do.           3      1   \n",
      "862255               Very light weight, barely hold paper           1      1   \n",
      "633612                          Only for 5 sheets or less           1      1   \n",
      "736944  Not happy with these. I am not able to print w...           1      1   \n",
      "\n",
      "                                                   review  \n",
      "98510   These do not work for refill ink it states one...  \n",
      "233824  Once the ink cartriges were installed, things ...  \n",
      "877033  Nothing special to say here - they are postcar...  \n",
      "929541               It is a machine for counting bills.?  \n",
      "201881  I guess you get what you pay for.  The magenta...  \n",
      "456205               Not the best quality. Pale in color.  \n",
      "645041               They do what they're supposed to do.  \n",
      "862255               Very light weight, barely hold paper  \n",
      "633612                          Only for 5 sheets or less  \n",
      "736944  Not happy with these. I am not able to print w...  \n"
     ]
    }
   ],
   "source": [
    "# Converting into binary classification problem\n",
    "df['label'] = df['star_rating'].apply(lambda x: 1 if x in [1,2,3] else 2)\n",
    "df['review'] = df['review_body']\n",
    "\n",
    "# Selecting 50,000 random reviews from each rating class\n",
    "# Randomizing to avoid biases \n",
    "df_class_1 = df[df['label'] == 1].sample(n=50000, random_state=55)\n",
    "df_class_2 = df[df['label'] == 2].sample(n=50000, random_state=55)\n",
    "\n",
    "# Creating a new df concatenating both classes \n",
    "balanced_df = pd.concat([df_class_1, df_class_2])\n",
    "\n",
    "print(balanced_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length before data cleaning:189.4582, Average length after data cleaning:179.7509\n"
     ]
    }
   ],
   "source": [
    "average_len_before = balanced_df['review'].str.len().mean()\n",
    "\n",
    "# 1)converting all reviews into lowercase. \n",
    "# Ensures consistency: \"Hello\" and \"hello\" are now the same. \n",
    "balanced_df['review'] = balanced_df['review'].str.lower()\n",
    "\n",
    "# 2)removing the HTML and URLs from the reviews\n",
    "# HTML/URLs don't provide valuable information for sentiment analysis, so we remove them. \n",
    "balanced_df['review'] = balanced_df['review'].str.replace(r'<.*?>', '', regex=True)\n",
    "balanced_df['review'] = balanced_df['review'].str.replace(r'http\\S+', '', regex=True)\n",
    "\n",
    "# 5)performing contractions on the reviews\n",
    "    # Need to process this before before removing non-alphanum chars and extra spaces \n",
    "# This task provides uniformity and simplifies tokenization\n",
    "def contractions_helper(ss):\n",
    "\n",
    "    # To avoid attributionError\n",
    "    if type(ss) != str: \n",
    "        return\n",
    "    contractions_dict = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"coudn't\": \"could not\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\"\n",
    "    }\n",
    "    # loop through the string and replace all contractions\n",
    "    for cont, exp in contractions_dict.items():\n",
    "            if cont in ss:\n",
    "                 ss = ss.replace(cont, exp)\n",
    "    return ss\n",
    "\n",
    "balanced_df['review'] = balanced_df['review'].apply(contractions_helper)\n",
    "\n",
    "# 3)removing non-alphabetical characters\n",
    "# Removing non-alphanum characters since they could be noise in sentiment analysis\n",
    "balanced_df['review'] = balanced_df['review'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "\n",
    "# 4)removing extra spaces\n",
    "balanced_df['review'] = balanced_df['review'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "\n",
    "average_len_after = balanced_df['review'].str.len().mean()\n",
    "\n",
    "# average length decreased after cleaning due to the removal of unwanted characters, spaces, and expansion of contractions\n",
    "print(f'Average length before data cleaning:{average_len_before:.4f}, Average length after data cleaning:{average_len_after:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jadriantan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jadriantan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jadriantan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# installation\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')  # This is for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Loading stopwords only once\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    " \n",
    "def removeStopWords(ss, english_stopwords):\n",
    "    # Tokenization\n",
    "    ss = nltk.word_tokenize(ss)\n",
    "\n",
    "    # Preprocessing stopwords\n",
    "    ss = [s for s in ss if s not in english_stopwords]\n",
    "\n",
    "    return ' '.join(ss)\n",
    "\n",
    "# Filtering out rows where 'review' is not a string\n",
    "balanced_df = balanced_df[balanced_df['review'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Removing stopwords in reviews\n",
    "balanced_df['review_no_stopwords'] = balanced_df['review'].apply(lambda x: removeStopWords(x, english_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98510     work refill ink states one use state anywhere ...\n",
      "233824    ink cartriges installed things seem functionin...\n",
      "877033           nothing special say postcards got expected\n",
      "929541                               machine counting bills\n",
      "201881    guess get pay magenta cartridge less prints ba...\n",
      "Name: review_no_stopwords, dtype: object\n",
      "98510     work refill ink state one use state anywhere site\n",
      "233824    ink cartriges installed thing seem functioning...\n",
      "877033            nothing special say postcard got expected\n",
      "929541                                machine counting bill\n",
      "201881    guess get pay magenta cartridge le print barel...\n",
      "Name: review_lemmatized, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Performing lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(ss):\n",
    "    ss = nltk.word_tokenize(ss)\n",
    "    ss = [lemmatizer.lemmatize(s) for s in ss]\n",
    "    return ' '.join(ss)\n",
    "\n",
    "balanced_df['review_lemmatized'] = balanced_df['review_no_stopwords'].apply(lemmatization)\n",
    "\n",
    "print(balanced_df['review_no_stopwords'].head())\n",
    "print(balanced_df['review_lemmatized'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length before preprocessing:115.6553, Average length after preprocessing:113.7376\n"
     ]
    }
   ],
   "source": [
    "# Double checking everything\n",
    "\n",
    "average_len_before_lemmatization = balanced_df['review_no_stopwords'].str.len().mean()\n",
    "average_len_after_lemmatization = balanced_df['review_lemmatized'].str.len().mean()\n",
    "\n",
    "print(f'Average length before preprocessing:{average_len_before_lemmatization:.4f}, Average length after preprocessing:{average_len_after_lemmatization:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model trained with amazon reviews\n",
    "sentences = balanced_df['review_lemmatized'].str.split().tolist() # get review sentences\n",
    "# train word2vec model\n",
    "my_model = Word2Vec(sentences, vector_size=300, window=13, min_count=9, workers=4)\n",
    "my_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "# Pre-trained word2vec model\n",
    "wv_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Model: Similarity for words 'excellent' and 'outstanding': 0.55674857\n",
      "Word2Vec Model: King - Man + Woman =  queen\n",
      "My Model: Similarity between 'excellent' and 'outstanding': 0.61221415\n",
      "My Model: King - Man + Woman =  diary\n"
     ]
    }
   ],
   "source": [
    "# Pretrained: Check for semantic similarities\n",
    "print(\"Word2Vec Model: Similarity for words 'excellent' and 'outstanding':\", wv_model.similarity('excellent', 'outstanding'))\n",
    "\n",
    "# Pretrained: Check for analogy: King - Man + Woman\n",
    "result = wv_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(\"Word2Vec Model: King - Man + Woman = \", result[0][0])\n",
    "\n",
    "\n",
    "# My model: Check semantic similarities\n",
    "if 'excellent' in my_model.wv and 'outstanding' in my_model.wv:\n",
    "    print(\"My Model: Similarity between 'excellent' and 'outstanding':\", my_model.wv.similarity('excellent', 'outstanding'))\n",
    "\n",
    "# My model: Check analogy: King - Man + Woman\n",
    "if all(word in my_model.wv for word in ['woman', 'king', 'man']):\n",
    "    result = my_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "    print(\"My Model: King - Man + Woman = \", result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Models: Perceptron and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# TFIDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TFIDF Features\n",
    "tfidf_vector = TfidfVectorizer(max_features=5000)\n",
    "tfidf_features = tfidf_vector.fit_transform(balanced_df['review_lemmatized'])\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
    "    tfidf_features, \n",
    "    balanced_df['label'],\n",
    "    test_size= 0.2,\n",
    "    random_state=55\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Features\n",
    "\n",
    "# Calculate average Word2Vec for each review\n",
    "def average_word2vec(review, model, dimension):\n",
    "    avg_w2v = np.zeros((dimension,))\n",
    "    num_words = 0\n",
    "    for word in review:\n",
    "        if word in model:  \n",
    "            avg_w2v += model[word]  \n",
    "            num_words += 1\n",
    "    if num_words > 0:\n",
    "        avg_w2v /= num_words\n",
    "    return avg_w2v\n",
    "\n",
    "\n",
    "# Convert reviews into feature vectors using average Word2Vec\n",
    "word2vec_features = np.array([average_word2vec(review, wv_model, 300) for review in sentences])\n",
    "\n",
    "# Splitting into train and test sets\n",
    "x_train_word2vec, x_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(\n",
    "    word2vec_features, \n",
    "    balanced_df['label'],\n",
    "    test_size= 0.2,\n",
    "    random_state=55\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron w/ TFIDF features and word2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Perceptron Model~ Precision:0.7628, Recall:0.8266, F1-Score:0.7934\n",
      "Word2Vec Perceptron Model~ Precision:0.6936, Recall:0.8967, F1-Score:0.7822\n"
     ]
    }
   ],
   "source": [
    "# Perceptron on tfidf features\n",
    "tfidf_perceptron = Perceptron(max_iter=1000, random_state=55)\n",
    "\n",
    "# Fit training set into Perceptron Model \n",
    "tfidf_perceptron.fit(x_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Make predictions w/ testing set\n",
    "tfidf_prediction = tfidf_perceptron.predict(x_test_tfidf)\n",
    "\n",
    "# Report Precision, Recall, and f1-score\n",
    "tfidf_test_precision = precision_score(y_test_tfidf, tfidf_prediction, average='binary')\n",
    "tfidf_test_recall = recall_score(y_test_tfidf, tfidf_prediction, average='binary')\n",
    "tfidf_test_f1 = f1_score(y_test_tfidf, tfidf_prediction, average='binary')\n",
    "\n",
    "# Print\n",
    "print(f\"TF-IDF Perceptron Model~ Precision:{tfidf_test_precision:.4f}, Recall:{tfidf_test_recall:.4f}, F1-Score:{tfidf_test_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Perceptron on word2vec features\n",
    "word2vec_perceptron = Perceptron(max_iter=1000, random_state=55)\n",
    "\n",
    "# Fit training set into Perceptron Model\n",
    "word2vec_perceptron.fit(x_train_word2vec, y_train_word2vec)\n",
    "\n",
    "# Make predictions w/ testing set\n",
    "word2vec_prediction = word2vec_perceptron.predict(x_test_word2vec)\n",
    "\n",
    "# Report Precision, Recall, and f1-score\n",
    "word2vec_test_precision = precision_score(y_test_word2vec, word2vec_prediction, average='binary')\n",
    "word2vec_test_recall = recall_score(y_test_word2vec, word2vec_prediction, average='binary')\n",
    "word2vec_test_f1 = f1_score(y_test_word2vec, word2vec_prediction, average='binary')\n",
    "\n",
    "# Print\n",
    "print(f\"Word2Vec Perceptron Model~ Precision:{word2vec_test_precision:.4f}, Recall:{word2vec_test_recall:.4f}, F1-Score:{word2vec_test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My explanation:\n",
    "- Deep learning models perform better with embeddings like Word2Vec than with sparse representations like TF-IDF. \n",
    "- Linear models like perceptrons might sometimes favor the discriminative power of TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_train_tensor: tensor([1, 2])\n",
      "Unique values in y_test_tensor: tensor([1, 2])\n",
      "Shape of X_train_tensor: torch.Size([79988, 300])\n",
      "Shape of X_test_tensor: torch.Size([19998, 300])\n",
      "Updated unique values in y_train_tensor: tensor([0, 1])\n",
      "Updated unique values in y_test_tensor: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(x_train_word2vec)\n",
    "y_train_tensor = torch.LongTensor(y_train_word2vec.values) \n",
    "X_test_tensor = torch.FloatTensor(x_test_word2vec)\n",
    "y_test_tensor = torch.LongTensor(y_test_word2vec.values)\n",
    "\n",
    "# Check before moving forward\n",
    "print(\"Unique values in y_train_tensor:\", torch.unique(y_train_tensor))\n",
    "print(\"Unique values in y_test_tensor:\", torch.unique(y_test_tensor))\n",
    "\n",
    "print(\"Shape of X_train_tensor:\", X_train_tensor.shape)\n",
    "# print(\"Sample values from X_train_tensor:\", X_train_tensor[:5])\n",
    "print(\"Shape of X_test_tensor:\", X_test_tensor.shape)\n",
    "# print(\"Sample values from X_test_tensor:\", X_test_tensor[:5])\n",
    "\n",
    "y_train_tensor -= 1\n",
    "y_test_tensor -= 1\n",
    "\n",
    "print(\"Updated unique values in y_train_tensor:\", torch.unique(y_train_tensor))\n",
    "print(\"Updated unique values in y_test_tensor:\", torch.unique(y_test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.48984336853027344\n",
      "Epoch 2/10, Loss: 0.4686760902404785\n",
      "Epoch 3/10, Loss: 0.49770113825798035\n",
      "Epoch 4/10, Loss: 0.5088074207305908\n",
      "Epoch 5/10, Loss: 0.4138981103897095\n",
      "Epoch 6/10, Loss: 0.4942331612110138\n",
      "Epoch 7/10, Loss: 0.45443031191825867\n",
      "Epoch 8/10, Loss: 0.4877156615257263\n",
      "Epoch 9/10, Loss: 0.4376051723957062\n",
      "Epoch 10/10, Loss: 0.425417423248291\n",
      "Test Accuracy: 82.60%\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(300, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    _, predicted = test_outputs.max(1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_word2vec)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the first 10 Word2Vec vectors for each review as the input feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
