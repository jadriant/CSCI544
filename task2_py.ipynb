{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMnoaCj23JXAjTtMB8yeq6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jadriant/CSCI544/blob/main/task2_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjq_6UpG-cMb",
        "outputId": "d8a8c9fa-6ac5-4822-9eba-04db9fae9c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "id": "RyqdoWTK-hXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "vT6Lpqj--jJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
      ],
      "metadata": {
        "id": "fJsMn6TK-lBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "PtC_pp82-riw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "0CduAlEJ-tAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Vocabulary"
      ],
      "metadata": {
        "id": "P5J8GmZw-2Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "\n",
        "word_frequency = Counter(itertools.chain(*dataset['train']['tokens']))  # type: ignore\n",
        "\n",
        "# Remove words below threshold 3\n",
        "word_frequency = {\n",
        "    word: frequency\n",
        "    for word, frequency in word_frequency.items()\n",
        "    if frequency >= 3\n",
        "}\n",
        "\n",
        "word2idx = {\n",
        "    word: index\n",
        "    for index, word in enumerate(word_frequency.keys(), start=2)\n",
        "}\n",
        "\n",
        "word2idx['[PAD]'] = 0\n",
        "word2idx['[UNK]'] = 1"
      ],
      "metadata": {
        "id": "czJITgxI-tbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize to ids"
      ],
      "metadata": {
        "id": "55g6ZMDv-45M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = (\n",
        "    dataset\n",
        "    .map(lambda x: {\n",
        "            'input_ids': [\n",
        "                word2idx.get(word, word2idx['[UNK]'])\n",
        "                for word in x['tokens']\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        ")\n",
        "\n",
        "dataset['train']['input_ids'][:3]"
      ],
      "metadata": {
        "id": "-Pa3U_Mg-zC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename 'ner_tags' to labels\n",
        "dataset = dataset.rename_column(\"ner_tags\", \"labels\")\n",
        "\n",
        "# Remove 'pos_tag' and 'chunk_tags'\n",
        "dataset = dataset.remove_columns([\"pos_tags\", \"chunk_tags\"])\n",
        "\n",
        "# Check before moving on\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "qLiJxhdo-74u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GloVe Embedding\n",
        "- also taking into account case sensitive"
      ],
      "metadata": {
        "id": "n8SisIczjJ0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to load GloVe embeddings\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    embeddings = np.zeros((len(word2idx), embedding_dim))\n",
        "    # Create a mapping for lowercased words to their GloVe vectors\n",
        "    glove_index = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            glove_index[word] = vector\n",
        "\n",
        "    # Assign GloVe vector to the word if present, otherwise use the lowercase version from GloVe\n",
        "    for word, idx in word2idx.items():\n",
        "        vector = glove_index.get(word)\n",
        "        if vector is not None:\n",
        "            embeddings[idx] = vector\n",
        "        else:\n",
        "            # Use the lowercase version if the case-sensitive version is not found\n",
        "            lowercase_vector = glove_index.get(word.lower())\n",
        "            if lowercase_vector is not None:\n",
        "                embeddings[idx] = lowercase_vector\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "glove_path = 'glove.6B.100d.txt'\n",
        "embedding_dim = 100  # Dimensionality of GloVe vectors\n",
        "\n",
        "# Load the embeddings\n",
        "glove_embeddings = load_glove_embeddings(glove_path, word2idx, embedding_dim)"
      ],
      "metadata": {
        "id": "oQIiY4g3jJXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture: BiLSTM Model\n",
        "\n",
        "Embedding dim 100 \\\n",
        "Num LSTM layers 1 \\\n",
        "LSTM hidden dim 256 \\\n",
        "LSTM Dropout 0.33 \\\n",
        "Linear output dim 128"
      ],
      "metadata": {
        "id": "GMk597aSAO2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "XQWu9RxaAP49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMForNER(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, lstm_hidden_dim, linear_output_dim, num_labels, dropout, pretrained_embeddings):\n",
        "        super(BiLSTMForNER, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False  # Freeze the embeddings\n",
        "\n",
        "        self.bilstm = nn.LSTM(embedding_dim, lstm_hidden_dim // 2, num_layers=1,\n",
        "                              bidirectional=True, batch_first=True, dropout=dropout)\n",
        "        self.linear = nn.Linear(lstm_hidden_dim, linear_output_dim)\n",
        "        self.elu = nn.ELU()\n",
        "        self.classifier = nn.Linear(linear_output_dim, num_labels)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "        bilstm_output, _ = self.bilstm(embeddings)\n",
        "        linear_output = self.elu(self.linear(bilstm_output))\n",
        "        logits = self.classifier(linear_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "wCaBACiIARdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_label(dataset):\n",
        "    max_label = 0\n",
        "    for split in dataset:\n",
        "        split_max = max([max(labels) for labels in dataset[split]['labels']])\n",
        "        max_label = max(max_label, split_max)\n",
        "    return max_label + 1  # Adding 1 because labels are zero-indexed\n",
        "\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "num_labels = get_max_label(dataset)\n",
        "\n",
        "model = BiLSTMForNER(\n",
        "    vocab_size=len(word2idx),\n",
        "    embedding_dim=100,  # The dimensionality of GloVe vectors\n",
        "    lstm_hidden_dim=256,  # LSTM hidden layer dimensionality\n",
        "    linear_output_dim=128,  # Linear layer output dimensionality\n",
        "    num_labels=num_labels,  # The number of labels in your dataset\n",
        "    dropout=0.33,  # The dropout rate for LSTM\n",
        "    pretrained_embeddings=glove_embeddings\n",
        ")\n",
        "\n",
        "# Define an optimizer and a loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Checking the number of labels\n",
        "print(num_labels)"
      ],
      "metadata": {
        "id": "-J8kHVDlAUmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pad sequences and create TensorDataset\n",
        "def create_dataset2(input_ids, labels, pad_label_value=-100):\n",
        "    # Pad the input sequences and the labels\n",
        "    input_ids_padded = pad_sequence([torch.tensor(s) for s in input_ids],\n",
        "                                    batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence([torch.tensor(l) for l in labels],\n",
        "                                 batch_first=True, padding_value=pad_label_value)\n",
        "    return TensorDataset(input_ids_padded, labels_padded)\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = create_dataset2(dataset['train']['input_ids'], dataset['train']['labels'])\n",
        "val_dataset = create_dataset2(dataset['validation']['input_ids'], dataset['validation']['labels'])\n",
        "\n",
        "# Create DataLoader objects\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "XqYB0Lv6Abb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to GPU (cuda) if available, otherwise stick with CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Move the model to the specified device\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        # Transfer batch to the device\n",
        "        input_ids, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids)\n",
        "        loss = loss_fn(outputs.view(-1, num_labels), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch} total loss: {total_loss}\")\n",
        "\n",
        "    # Evaluation Loop\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # Transfer batch to the device\n",
        "            input_ids, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            loss = loss_fn(outputs.view(-1, num_labels), labels.view(-1))\n",
        "            total_eval_loss += loss.item()\n",
        "    print(f\"Validation loss: {total_eval_loss}\")\n",
        "\n",
        "    if epoch > 10 and float(total_loss) < 1.0:\n",
        "      break"
      ],
      "metadata": {
        "id": "bF_DSw1kBWO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of NER tags\n",
        "ner_tags = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "# Create ner_to_index and index_to_tag dictionaries\n",
        "ner_to_index = {tag: index for index, tag in enumerate(ner_tags)}\n",
        "index_to_tag = {index: tag for tag, index in ner_to_index.items()}"
      ],
      "metadata": {
        "id": "4xYrWmnABfbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from conlleval import evaluate\n",
        "import itertools\n",
        "\n",
        "# Evaluation Loop\n",
        "model.eval()\n",
        "all_true_tags = []\n",
        "all_pred_tags = []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        # Transfer batch to the device\n",
        "        input_ids, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        outputs = model(input_ids)\n",
        "\n",
        "        # Get the model's predictions\n",
        "        predictions = torch.argmax(outputs, dim=2)\n",
        "\n",
        "        # Exclude padding from evaluation\n",
        "        for i in range(labels.size(0)):  # Batch size\n",
        "            true_labels = labels[i]\n",
        "            pred_labels = predictions[i]\n",
        "            for j in range(true_labels.size(0)):  # Sequence length\n",
        "                if true_labels[j] != -100:  # Assuming -100 is used for padding\n",
        "                    true_tag = index_to_tag[true_labels[j].item()]\n",
        "                    pred_tag = index_to_tag[pred_labels[j].item()]\n",
        "                    all_true_tags.append(true_tag)\n",
        "                    all_pred_tags.append(pred_tag)\n",
        "\n",
        "# Evaluate with conlleval\n",
        "prec, rec, f1 = evaluate(all_true_tags, all_pred_tags, verbose=True)\n",
        "print(f'Precision: {prec}, Recall: {rec}, F1 Score: {f1}')"
      ],
      "metadata": {
        "id": "F9NHwboKBh3d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}