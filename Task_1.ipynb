{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyUi4PSXB8nSY+4AyyMrHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jadriant/CSCI544/blob/main/Task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjq_6UpG-cMb",
        "outputId": "2be31b42-a3f2-4e0c-aff1-ba9dc42eff5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyqdoWTK-hXA",
        "outputId": "1b3851ae-42e3-43ec-b943-841e617946ba"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-10 04:12:44--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-11-10 04:12:44--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-11-10 04:12:44--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2023-11-10 04:15:23 (5.17 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT6Lpqj--jJ9",
        "outputId": "3e067f58-0fc2-49f6-83cf-7476fdaa1c94"
      },
      "execution_count": 54,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJsMn6TK-lBo",
        "outputId": "58df00ce-56e9-46e8-bf3c-ce8aa72c23d9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-10 04:18:32--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7502 (7.3K) [text/plain]\n",
            "Saving to: ‘conlleval.py.2’\n",
            "\n",
            "\rconlleval.py.2        0%[                    ]       0  --.-KB/s               \rconlleval.py.2      100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-10 04:18:32 (92.2 MB/s) - ‘conlleval.py.2’ saved [7502/7502]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "PtC_pp82-riw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "0CduAlEJ-tAf"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Vocabulary"
      ],
      "metadata": {
        "id": "P5J8GmZw-2Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "\n",
        "word_frequency = Counter(itertools.chain(*dataset['train']['tokens']))  # type: ignore\n",
        "\n",
        "# Remove words below threshold 3\n",
        "word_frequency = {\n",
        "    word: frequency\n",
        "    for word, frequency in word_frequency.items()\n",
        "    if frequency >= 3\n",
        "}\n",
        "\n",
        "word2idx = {\n",
        "    word: index\n",
        "    for index, word in enumerate(word_frequency.keys(), start=2)\n",
        "}\n",
        "\n",
        "word2idx['[PAD]'] = 0\n",
        "word2idx['[UNK]'] = 1"
      ],
      "metadata": {
        "id": "czJITgxI-tbL"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize to ids"
      ],
      "metadata": {
        "id": "55g6ZMDv-45M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = (\n",
        "    dataset\n",
        "    .map(lambda x: {\n",
        "            'input_ids': [\n",
        "                word2idx.get(word, word2idx['[UNK]'])\n",
        "                for word in x['tokens']\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        ")\n",
        "\n",
        "dataset['train']['input_ids'][:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Pa3U_Mg-zC0",
        "outputId": "fcc63597-a69f-4a7a-9104-0686f1b6205a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1, 3, 4, 5, 6, 7, 8, 9], [10, 11], [12, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename 'ner_tags' to labels\n",
        "dataset = dataset.rename_column(\"ner_tags\", \"labels\")\n",
        "\n",
        "# Remove 'pos_tag' and 'chunk_tags'\n",
        "dataset = dataset.remove_columns([\"pos_tags\", \"chunk_tags\"])\n",
        "\n",
        "# Check before moving on\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLiJxhdo-74u",
        "outputId": "56b6ea0d-d310-4d95-e768-ee0a6a0fe66f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'tokens', 'labels', 'input_ids'],\n",
            "        num_rows: 14041\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'tokens', 'labels', 'input_ids'],\n",
            "        num_rows: 3250\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'tokens', 'labels', 'input_ids'],\n",
            "        num_rows: 3453\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture: BiLSTM Model\n",
        "\n",
        "Embedding dim 100 \\\n",
        "Num LSTM layers 1 \\\n",
        "LSTM hidden dim 256 \\\n",
        "LSTM Dropout 0.33 \\\n",
        "Linear output dim 128"
      ],
      "metadata": {
        "id": "GMk597aSAO2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "XQWu9RxaAP49"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMForNER(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, lstm_hidden_dim, linear_output_dim, num_labels, dropout=0.33):\n",
        "        super(BiLSTMForNER, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bilstm = nn.LSTM(embedding_dim, lstm_hidden_dim // 2, num_layers=1,\n",
        "                              bidirectional=True, batch_first=True, dropout=dropout)\n",
        "        self.linear = nn.Linear(lstm_hidden_dim, linear_output_dim)\n",
        "        self.elu = nn.ELU()\n",
        "        self.classifier = nn.Linear(linear_output_dim, num_labels)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "        bilstm_output, _ = self.bilstm(embeddings)\n",
        "        linear_output = self.elu(self.linear(bilstm_output))\n",
        "        logits = self.classifier(linear_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "wCaBACiIARdR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_input_ids = set()\n",
        "# for split in ['train', 'validation', 'test']:\n",
        "#     for input_id_list in dataset[split]['input_ids']:\n",
        "#         unique_input_ids.update(input_id_list)\n",
        "\n",
        "def get_max_label(dataset):\n",
        "    max_label = 0\n",
        "    for split in dataset:\n",
        "        split_max = max([max(labels) for labels in dataset[split]['labels']])\n",
        "        max_label = max(max_label, split_max)\n",
        "    return max_label + 1  # Adding 1 because labels are zero-indexed\n",
        "\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "num_labels = get_max_label(dataset)\n",
        "\n",
        "model = BiLSTMForNER(\n",
        "    vocab_size,\n",
        "    embedding_dim=100,\n",
        "    lstm_hidden_dim=256,\n",
        "    linear_output_dim=128,\n",
        "    num_labels=num_labels,\n",
        "    dropout=0.33\n",
        "    )\n",
        "\n",
        "# Define an optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Checking the number of labels\n",
        "print(num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J8kHVDlAUmP",
        "outputId": "0c296918-67c6-414c-c478-2f51b52c9696"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.33 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pad sequences and create TensorDataset\n",
        "def create_dataset2(input_ids, labels, pad_label_value=-100):\n",
        "    # Pad the input sequences and the labels\n",
        "    input_ids_padded = pad_sequence([torch.tensor(s) for s in input_ids],\n",
        "                                    batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence([torch.tensor(l) for l in labels],\n",
        "                                 batch_first=True, padding_value=pad_label_value)\n",
        "    return TensorDataset(input_ids_padded, labels_padded)\n",
        "\n",
        "train_dataset = create_dataset2(dataset['train']['input_ids'], dataset['train']['labels'])\n",
        "val_dataset = create_dataset2(dataset['validation']['input_ids'], dataset['validation']['labels'])\n",
        "\n",
        "# Create DataLoader objects\n",
        "batch_size = 32\n",
        "num_epochs = 40\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "XqYB0Lv6Abb7"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to GPU (cuda) if available, otherwise stick with CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Move the model to the specified device\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        # Transfer batch to the device\n",
        "        input_ids, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids)\n",
        "        loss = loss_fn(outputs.view(-1, num_labels), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch} total loss: {total_loss}\")\n",
        "\n",
        "    # Evaluation Loop\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # Transfer batch to the device\n",
        "            input_ids, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            loss = loss_fn(outputs.view(-1, num_labels), labels.view(-1))\n",
        "            total_eval_loss += loss.item()\n",
        "    print(f\"Validation loss: {total_eval_loss}\")\n",
        "\n",
        "    if epoch > 10 and float(total_loss) < 1.0:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF_DSw1kBWO1",
        "outputId": "450035c1-86cf-4c53-e6df-a05e0c913d41"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Epoch 1 total loss: 0.7959661375311953\n",
            "Validation loss: 41.22647560811069\n",
            "Epoch 2 total loss: 0.8046723342886253\n",
            "Validation loss: 38.85210501128768\n",
            "Epoch 3 total loss: 0.7769911385724981\n",
            "Validation loss: 39.608064562282834\n",
            "Epoch 4 total loss: 0.807534541399832\n",
            "Validation loss: 40.8617992223962\n",
            "Epoch 5 total loss: 0.8615019235458021\n",
            "Validation loss: 41.15451389513774\n",
            "Epoch 6 total loss: 2.5866465404233168\n",
            "Validation loss: 37.80479847879906\n",
            "Epoch 7 total loss: 1.987935914317859\n",
            "Validation loss: 37.034843524097596\n",
            "Epoch 8 total loss: 0.8977204087004793\n",
            "Validation loss: 37.86874765544053\n",
            "Epoch 9 total loss: 0.7332256262634473\n",
            "Validation loss: 38.85467227516983\n",
            "Epoch 10 total loss: 0.7119645048001075\n",
            "Validation loss: 39.50379347663966\n",
            "Epoch 11 total loss: 0.7155515567244493\n",
            "Validation loss: 39.47706316190761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of NER tags\n",
        "ner_tags = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "# Create ner_to_index and index_to_tag dictionaries\n",
        "ner_to_index = {tag: index for index, tag in enumerate(ner_tags)}\n",
        "index_to_tag = {index: tag for tag, index in ner_to_index.items()}"
      ],
      "metadata": {
        "id": "4xYrWmnABfbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from conlleval import evaluate\n",
        "import itertools\n",
        "\n",
        "# Evaluation Loop\n",
        "model.eval()\n",
        "all_true_tags = []\n",
        "all_pred_tags = []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        # Transfer batch to the device\n",
        "        input_ids, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        outputs = model(input_ids)\n",
        "\n",
        "        # Get the model's predictions\n",
        "        predictions = torch.argmax(outputs, dim=2)\n",
        "\n",
        "        # Exclude padding from evaluation\n",
        "        for i in range(labels.size(0)):  # Batch size\n",
        "            true_labels = labels[i]\n",
        "            pred_labels = predictions[i]\n",
        "            for j in range(true_labels.size(0)):  # Sequence length\n",
        "                if true_labels[j] != -100:  # Assuming -100 is used for padding\n",
        "                    true_tag = index_to_tag[true_labels[j].item()]\n",
        "                    pred_tag = index_to_tag[pred_labels[j].item()]\n",
        "                    all_true_tags.append(true_tag)\n",
        "                    all_pred_tags.append(pred_tag)\n",
        "\n",
        "# Evaluate with conlleval\n",
        "prec, rec, f1 = evaluate(all_true_tags, all_pred_tags, verbose=True)\n",
        "print(f'Precision: {prec}, Recall: {rec}, F1 Score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9NHwboKBh3d",
        "outputId": "4f09e011-311a-4b3a-b1a4-1583e28bb1cc"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51362 tokens with 5942 phrases; found: 5723 phrases; correct: 4581.\n",
            "accuracy:  79.41%; (non-O)\n",
            "accuracy:  95.71%; precision:  80.05%; recall:  77.10%; FB1:  78.54\n",
            "              LOC: precision:  87.85%; recall:  85.03%; FB1:  86.42  1778\n",
            "             MISC: precision:  76.28%; recall:  74.30%; FB1:  75.27  898\n",
            "              ORG: precision:  75.24%; recall:  70.25%; FB1:  72.66  1252\n",
            "              PER: precision:  77.55%; recall:  75.57%; FB1:  76.55  1795\n",
            "Precision: 80.0454307181548, Recall: 77.09525412319084, F1 Score: 78.54264894984998\n"
          ]
        }
      ]
    }
  ]
}